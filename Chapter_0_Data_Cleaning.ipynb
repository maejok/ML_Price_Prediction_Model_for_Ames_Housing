{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component, due one week into the project, requires students to submit a simplistic model (MODEL1) that can be used for predicting the sale price of houses. This component is used to verify that students understand the assignment and are familiar with the methodology for submitting their models. The second component, due two\n",
    "weeks into the project, requires students to submit a more complex model (MODEL2) that represents their best effort at predicting housing prices. This component will be applied to a validation set to determine a “fit” grade that comprises 30% of their project grade. The final component, due on the last day of class, is a written report that contains all the analysis, interpretation, and information for the two submitted models. The written report completes the remaining 70% of the total project grade.\n",
    "MODEL2 is evaluated through a cross-validation or data splitting technique where the original data set is split into two data sets: the training set and the validation set. The students are given the training set for the purpose of developing their model and I retain the validation set for use in evaluating their model. \n",
    "\n",
    " I chose to use randomization to create my Boston sets but those wishing to achieve a more consistent split may want to use a systematic sampling scheme. Simply order the original data set by a variable of interest (such as sale price) and select every kth observation to achieve the desired sample size (k=2 for a 50/50 split or k=4 for a 75/25 split).\n",
    " The most common error I have found is students losing track of what they have done in creating complex variables such as transformations and interactions (i.e. they think that their new variable v13 is an interaction between v1 and v3 when in actuality it is some other combination or transformation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remind the students of the concept of the validation set (mentioned earlier in the semester) and then talk about the four main criteria I use for evaluating their model. In each measure, the actual home price (Y) of each observation in the validation set is compared the predicted value (Yhat) obtained from their model.\n",
    "\n",
    " Bias –  $\\Sigma (Yhat-Y)/N $– This concept is the easiest for the students to understand as positive values indicate the model tends to overestimate price (on average) while negative values indicate the model tends to underestimate price.\n",
    "\n",
    " Maximum Deviation - Max $|Y-Yhat|$ - Students also find this measure easy to understand as it identifies the worst prediction they made in the validation data set.\n",
    "\n",
    " Mean Absolute Deviation –$\\Sigma |Yhat-Y|/N $ - Although not as intuitive to the students, once contrasted with bias, students grasp that it is the average error (regardless of sign).\n",
    "\n",
    " Mean Square Error –$\\Sigma |Yhat-Y|^2/N $– The least intuitive and least meaningful measure for the students. I only include it so that I can compare its calculation to the methodology used to obtain the coefficient estimates from the original data set (linking back to the idea of Least Squares Regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "There are two data sets included in the data folder: `Ames_Housing_Price_Data.csv` and `Ames_Real_Estate_Data.csv`.\n",
    "\n",
    "The `Ames_Housing_Price_Data.csv` set contains $81$ data columns, including the key feature **SalePrice** which will be used as the target of the predictive/descriptive modeling. **PID** refers to the land parcel ID, which can merged on the *MapRefNo* column of the **Ames Accessor Data** (`Ames_Real_Estate_Data.csv`) to find the property address. Using a free service, such as **geopy**, we can find the long-lat coordinates of the houses.\n",
    "\n",
    "The columns of the data are mostly attributes associated with the land and the houses. There are size related attributes, quality and condition attributes, house attachment attributes, etc.\n",
    "\n",
    "To establish a foundation for your team's data analytics, we offer some insights on the house sizes vs. prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the datasets and the libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import statistics as stats\n",
    "realEstate = pd.read_csv(\"Ames_Real_Estate_Data.csv\")\n",
    "realEstate = realEstate[['MapRefNo','Prop_Addr','MA_Zip1']]\n",
    "geocode_data =pd.read_csv(\"geocode_data.csv\")\n",
    "\n",
    "housing = pd.read_csv('Ames_HousePrice.csv', index_col=0)\n",
    "housing = housing[housing.GrLivArea<3700]\n",
    "from scipy import stats\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.PID.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocode_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.merge(housing, geocode_data.iloc[:,1:6], how='left', left_on='PID', right_on =\"PID\")\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qual_related = housing.filter(regex='Qual$|Cond$').fillna(\"TA\")\n",
    "qual_related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qual_related.GarageCond.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_related.fillna(\"TA\", inplace=True)\n",
    "def Rating(t):\n",
    "    if t ==\"Ex\": return  7\n",
    "    elif t == \"Gd\": return 5\n",
    "    elif t == \"TA\": return 3\n",
    "    elif t == \"Fa\": return 2.5\n",
    "    elif t == \"Po\": return 1\n",
    "    else: return 0\n",
    "for ele in qual_related.iloc[:,2:]:\n",
    "    \n",
    "    housing[ele]=qual_related[ele].copy().map(Rating)\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UpSampling the Street labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.Street.value_counts()\n",
    "def Ratio(t):\n",
    "    if t == 'Pave': return 1.0\n",
    "   \n",
    "    else: return 180.0\n",
    "# the returned values must be integers   \n",
    "ratios = housing['Street'].map(Ratio)\n",
    "index_repeat = housing.index.repeat(ratios)\n",
    "index_repeat = pd.Series(index_repeat, name='repeat')\n",
    "index_repeat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "housing = housing.loc[index_repeat].Street.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.merge(index_repeat, housing, how='left', left_on='repeat', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ratio2(t):\n",
    "    if t ==\"Normal\": return  1.0\n",
    "    elif t == \"Partial\": return 4\n",
    "    elif t == \"Alloca\": return 5\n",
    "    elif t == \"Abnormal\": return 6\n",
    "    elif t == \"Family\": return 12\n",
    "    else: return 100\n",
    "\n",
    "ratios = housing['SaleCondition'].map(Ratio2)\n",
    "index_repeat = housing.index.repeat(ratios)\n",
    "index_repeat = pd.Series(index_repeat, name='repeat')\n",
    "index_repeat\n",
    "housing = pd.merge(index_repeat, housing, how='left', left_on='repeat', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking unique PID #s\n",
    "uni = housing.PID.unique()\n",
    "uni.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the prices vary by neighbourhood\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "\n",
    "housing.boxplot(column ='SalePrice', by = 'Neighborhood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Dummyfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the averge price by neighborhood\n",
    "dummy = housing.groupby(\"Neighborhood\")[[\"SalePrice\"]].mean()\n",
    "dummy.rename(columns = {\"SalePrice\":\"Price_by_hood\"}, inplace =True)\n",
    "dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "housing = pd.merge(housing, dummy, how='left', on=['Neighborhood', 'Neighborhood'])\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We trim the outliers from the list\n",
    "#housing = housing\n",
    "#leng = len(housing)\n",
    "#print(leng)\n",
    "#housing[\"Gradient\"] = (housing.SalePrice-15000)/(housing.GrLivArea)\n",
    "\n",
    "#housing=housing.sort_values(by=\"Gradient\")[(housing.sort_values(by=\"Gradient\")[\"Gradient\"]>30) & (housing.sort_values(by=\"Gradient\")[\"Gradient\"]<220)]\n",
    "#housing[\"Gradient2\"] = (housing.SalePrice)/(housing.GrLivArea-1600.01) \n",
    "#housing=housing.sort_values(by=\"Gradient2\")[ (housing.sort_values(by=\"Gradient2\")[\"Gradient2\"]>250)|(housing.sort_values(by=\"Gradient2\")[\"Gradient2\"]<0)]\n",
    "#housing[\"Gradient3\"] = (housing.SalePrice -100000)/(housing.TotalBsmtSF +1) \n",
    "#housing=housing.sort_values(by=\"Gradient3\")[ (housing.sort_values(by=\"Gradient3\")[\"Gradient3\"]<200)]\n",
    "#housing[\"Gradient4\"] = (housing.SalePrice)/(housing.TotalBsmtSF-1200.01) \n",
    "#housing=housing.sort_values(by=\"Gradient4\")[ (housing.sort_values(by=\"Gradient4\")[\"Gradient4\"]>300000/1300)|(housing.sort_values(by=\"Gradient4\")[\"Gradient4\"]<0)]\n",
    "\n",
    "#housing\n",
    "#leng2 = len(housing)\n",
    "#outlier_pct = 100*(leng-leng2)/leng\n",
    "#outlier_pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.iloc[:,65:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between the Price and some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size_related = housing.filter(regex='SF$|Area$')\n",
    "size_related.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_related.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_related = size_related.fillna(1)  # We fill the very minor missing values by 0.0\n",
    "F_values, p_values = f_regression(size_related, housing['SalePrice'])\n",
    "pd.Series(p_values, index=size_related.columns).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price        = housing['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr = pd.concat([size_related, housing['SalePrice']], axis=1).corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the heatmap, SalPrice has strong positive ralation with GrLivArea, TotalBsmtSF,GarageArea and negative relation with LowQuanlFnSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of the Features\n",
    "\n",
    "Most of the size related columns have significant p-values on their correlations with **SalePrice**. The **Gross Living Area** (GrLivArea) has a p-value of zero, which indicates a very strong statistical relationship. We will focus our research on **GrLivArea**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#housing['SalePrice'] = housing['SalePrice'].apply(lambda x: np.log(x))\n",
    "\n",
    "housing[['GrLivArea', 'SalePrice']].plot(kind='scatter', x='GrLivArea', y='SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing['SalePrice'] = housing['SalePrice'].apply(lambda x: np.log(x))\n",
    "\n",
    "housing[['TotalBsmtSF', 'SalePrice']].plot(kind='scatter', x='TotalBsmtSF', y='SalePrice')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the price against grLivArea\n",
    "lm = LinearRegression()\n",
    "grLivArea = size_related[['TotalBsmtSF', \"GrLivArea\"]]\n",
    "grLivArea.isna().sum()\n",
    "lm.fit(grLivArea, price)\n",
    "lm.score(grLivArea, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.intercept_, lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the price against qual_related\n",
    "qual_related = housing.filter(regex='Qual$|Cond$')\n",
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(qual_related, price)\n",
    "lm.score(qual_related, price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schematically, the above linear regression can be expressed as\n",
    "\n",
    "$$price = \\beta_0 +\\beta_1\\cdot grLivArea + \\epsilon = -31601.646+79\\cdot TotalBsmtSF +86\\cdot grLivArea \\epsilon$$\n",
    "\n",
    "This  formula explains 58% of the variation in the price for all the housing transactions.\n",
    "Overall, the size of the property explains 76.7 of the variation in the price while the quality explains 72 percent of the variation in the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing['SalePrice'] = housing['SalePrice'].apply(lambda x: np.log(x))\n",
    "\n",
    "plt.scatter( housing['GrLivArea'].apply(lambda x: np.log(x+2)), housing['TotalBsmtSF'].apply(lambda x: np.log(x+2)))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 3D scatter plot\n",
    "\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "N = len(price)\n",
    "beta = np.array([-31602, 78, 86])\n",
    "x_m = np.array(grLivArea.head(N)) #np.random.randn(N, 2)\n",
    "y_m = np.array(price.head(N))#np.dot(np.append(np.ones((N,1)), x_m, axis=1), beta) + np.random.randn(N)*4\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "# plot the data points\n",
    "X = np.array(list(map(lambda x: [3]+ list(x), x_m)))  # Idiomatic Py3, but inefficient on Py2\n",
    "up = np.where(y_m >= np.sum(X*beta, axis=1))[0]\n",
    "down = np.where(y_m < np.sum(X*beta, axis=1))[0]\n",
    "ax.scatter(x_m[up, 0], x_m[up, 1], y_m[up], c='green', alpha=.6)\n",
    "ax.scatter(x_m[down, 0], x_m[down, 1], y_m[down], c='blue', alpha=.4)\n",
    "\n",
    "# plot the error bars\n",
    "ax = fig.gca(projection='3d')\n",
    "x_up = x_m[up,:]; y_up = y_m[up]\n",
    "up_kwargs = dict(color='red', alpha=.6, lw=0.8)\n",
    "for i, j, k in zip(x_up[:, 0], x_up[:, 1], y_up):\n",
    "    ax.plot([i, i], [j, j], [k, np.dot(beta, [1, i, j])], **up_kwargs)\n",
    "    \n",
    "x_down = x_m[down,:]; y_down = y_m[down]\n",
    "down_kwargs = dict(color='red', alpha=.3, lw=0.8)\n",
    "for i, j, k in zip(x_down[:,0], x_down[:,1], y_down):\n",
    "    ax.plot([i, i], [j, j], [k, np.dot(beta, [1, i, j])], **down_kwargs)\n",
    "    \n",
    "    \n",
    "# plot the plane which represents the true model\n",
    "x_1 = np.linspace(min(x_m[:, 0])-.5, max(x_m[:, 0])+.5, 25)\n",
    "x_2 = np.linspace(min(x_m[:, 1])-.5, max(x_m[:, 1])+.5, 25)\n",
    "x_1, x_2 = np.meshgrid(x_1, x_2)\n",
    "x_3 = beta[1]*x_1 + beta[2]*x_2 + beta[0]\n",
    "surface_kwargs = dict(rstride=100, cstride=100, color='yellow', alpha=0.1)\n",
    "ax.plot_surface(x_1, x_2, x_3, **surface_kwargs)\n",
    "ax.set_xlabel('GrLivArea')\n",
    "ax.set_ylabel('TotalBsmtSF')\n",
    "ax.set_zlabel('SalePrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import linear_model\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(x_m, y_m)\n",
    "print(\"beta_1, beta_2: \" + str(np.round(ols.coef_, 3)))\n",
    "print(\"beta_0: \" + str(np.round(ols.intercept_, 3)))\n",
    "print(\"RSS: %.2f\" % np.sum((ols.predict(x_m) - y_m) ** 2))\n",
    "print(\"R^2: %.5f\" % ols.score(x_m, y_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction of numerical features with SalePrice"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#X_1 = X_bos.drop('CHAS', 1).drop('RAD', 1)\n",
    "X_1= size_related\n",
    "y_bos = housing[\"SalePrice\"]\n",
    "fig = plt.figure(figsize=(14, 33))\n",
    "gs = gridspec.GridSpec(12, 2)\n",
    "\n",
    "for i in range(12):\n",
    "    ax1 = plt.subplot(gs[i, 0])\n",
    "    ax2 = plt.subplot(gs[i, 1])    \n",
    "    sns.regplot(y_bos[X_1.iloc[:, i]>0], X_1.iloc[:, i][X_1.iloc[:, i]>0], ax=ax1)\n",
    "    ax1.set_title('{}'.format(X_1.columns[i]))\n",
    "    ax1.set_xlabel('')\n",
    "    ylim = ax1.get_ylim()   \n",
    "    X_1[X_1.columns[i]].hist(bins=50, ax=ax2, orientation='horizontal',color=\"g\")    \n",
    "    ax2.set_ylim((ylim[0], ylim[1]))\n",
    "    ax2.set_xlabel('')\n",
    "    ax2.set_xlim((0, 200))\n",
    "    for tick in ax2.yaxis.get_major_ticks():\n",
    "        tick.label1On = False\n",
    "        tick.label2On = True\n",
    "    if i != 0:\n",
    "        ax1.set_xticklabels([''])\n",
    "        ax2.set_xticklabels([''])\n",
    "    else:\n",
    "        ax1.set_title('SalePrice \\n', size=15)\n",
    "        ax2.set_title('count \\n', size=15)\n",
    "        for tick in ax1.xaxis.get_major_ticks():\n",
    "            tick.label1On = False\n",
    "            tick.label2On = True\n",
    "        for tick in ax2.xaxis.get_major_ticks():\n",
    "            tick.label1On = False\n",
    "            tick.label2On = True\n",
    "plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature to Feature Interaction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#X_1 = X_bos.drop('CHAS', 1).drop('RAD', 1)\n",
    "X_1= size_related\n",
    "L = len(X_1.columns)\n",
    "y_bos = housing[\"SalePrice\"]\n",
    "fig = plt.figure(figsize=(10, 35))\n",
    "gs = gridspec.GridSpec(L, 1)\n",
    "\n",
    "for i in range(L-1):\n",
    "    ax1 = plt.subplot(gs[i, 0])    \n",
    "    sns.regplot(X_1.iloc[:, i][X_1.iloc[:, i+1]>0], X_1.iloc[:, i+1][X_1.iloc[:, i+1]>0], ax=ax1)\n",
    "    ax1.set_title('{}'.format(X_1.columns[i+1]))\n",
    "    ax1.set_xlabel('{}'.format(X_1.columns[i]))\n",
    "    ylim = ax1.get_ylim()   \n",
    "plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df): \n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_pct = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_pct], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        return mis_val_table_ren_columns.sort_values(by= \"Missing Values\")[mis_val_table_ren_columns[\"Missing Values\"]>10] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(housing)[\"Missing Values\"].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing=housing.drop([\"PoolQC\",\"MiscFeature\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[housing.columns[housing.isnull().any()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the NA values which aren't NAs to different values to work better with the data set\n",
    "medl = housing.LotFrontage.median()\n",
    "medm = housing.MasVnrArea.median()\n",
    "housing.Alley = housing.Alley.fillna(\"No Alley Access\")\n",
    "housing.LotFrontage = housing.LotFrontage.fillna(medl)\n",
    "housing.MasVnrArea = housing.MasVnrArea.fillna(medm)\n",
    "housing.MasVnrType = housing.MasVnrType.fillna('None')\n",
    "housing.BsmtQual = housing.BsmtQual.fillna(\"No Basement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NA with the right values\n",
    "housing.BsmtCond = housing.BsmtCond.fillna(\"No Basement\")\n",
    "housing.BsmtExposure = housing.BsmtExposure.fillna(\"No Basement\")\n",
    "housing.BsmtFinType1 = housing.BsmtFinType1.fillna(\"No Basement\")\n",
    "housing.BsmtFinType2 = housing.BsmtFinType2.fillna(\"No Basement\")\n",
    "housing.FireplaceQu = housing.FireplaceQu.fillna(\"No Fireplace\")\n",
    "housing.GarageType = housing.GarageType.fillna(\"No Garage\")\n",
    "housing.GarageFinish = housing.GarageFinish.fillna(\"No Garage\")\n",
    "housing.GarageQual = housing.GarageQual.fillna(\"No Garage\")\n",
    "housing.GarageCond = housing.GarageCond.fillna(\"No Garage\")\n",
    "housing.Fence = housing.Fence.fillna(\"No Fence\")\n",
    "housing.Electrical = housing.Electrical.fillna(\"None\")\n",
    "\n",
    "med1 = housing.BsmtFinSF1.median()\n",
    "med2 = housing.BsmtFinSF2.median()\n",
    "medf = housing.BsmtUnfSF.median()\n",
    "medt = housing.TotalBsmtSF.median()\n",
    "meda = housing.GarageArea.median()\n",
    "medlon =housing.long.median()\n",
    "medlat = housing.lat.median()\n",
    "medist = housing.dist.median()\n",
    "medinc = housing.income.median()\n",
    "\n",
    "housing.BsmtFinSF1 = housing.BsmtFinSF1.fillna(med1)\n",
    "housing.BsmtFinSF2 = housing.BsmtFinSF2.fillna(med2)\n",
    "housing.BsmtUnfSF = housing.BsmtUnfSF.fillna(medf)\n",
    "housing.TotalBsmtSF = housing.TotalBsmtSF.fillna(medt)\n",
    "housing.GarageArea = housing.GarageArea.fillna(meda)\n",
    "housing.long = housing.long.fillna(medlon)\n",
    "housing.lat = housing.lat.fillna(medlat)\n",
    "housing.dist = housing.dist.fillna(medist)\n",
    "housing.income = housing.income.fillna(medinc)\n",
    "\n",
    "housing.BsmtFullBath = housing.BsmtFullBath.fillna(0.0)\n",
    "housing.BsmtHalfBath = housing.BsmtHalfBath.fillna(0.0)\n",
    "housing.GarageCars = housing.GarageCars.fillna(0.0)\n",
    "\n",
    "housing.GarageYrBlt = np.where(housing.GarageYrBlt.notnull(),housing.GarageYrBlt, housing.YearBuilt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.YearBuilt = 2010 - housing.YearBuilt\n",
    "housing.GarageYrBlt = 2010 - housing.GarageYrBlt\n",
    "housing.YrSold = 2010 -housing.YrSold \n",
    "housing.YearRemodAdd = 2010 -housing.YearRemodAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of new column combining full and half bathrooms into one\n",
    "bathrm = (housing['FullBath'] + housing['BsmtFullBath'] +\n",
    "(housing['HalfBath']*0.5) + (housing['BsmtHalfBath']*0.5))\n",
    "housing['bathrm_cnt'] = bathrm\n",
    "\n",
    "# Creation of new column combining deck/porch-related sq footage into one\n",
    "patioSF = (housing['WoodDeckSF'] + housing['OpenPorchSF']+ housing['EnclosedPorch'] + \n",
    "           housing['3SsnPorch'] + housing['ScreenPorch'])\n",
    "housing['patioSF'] = patioSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider removing this session if the R**2 does not improve\n",
    "\n",
    "# Zoning Dummy\n",
    "\n",
    "dummies = pd.get_dummies(housing, prefix=['MSZoning'], columns = ['MSZoning'], drop_first = True)\n",
    "dummies = dummies[['PID','MSZoning_RH','MSZoning_RL','MSZoning_RM']]\n",
    "housing = housing.merge(dummies, left_on = 'PID', right_on = 'PID')\n",
    "print(housing.shape)\n",
    "def near_rr(df):\n",
    "    rr = ['RRAe', 'RRAn', 'RRNn','RRNe']\n",
    "    if df['Condition1'] in rr:\n",
    "        return 1\n",
    "    if df['Condition2'] in rr:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "#housing = near_rr(housing)\n",
    "# Creating near RR column\n",
    "housing['NearRR'] = housing.apply(near_rr, axis =1)\n",
    "print(housing.shape)\n",
    "def near_pos(df):\n",
    "    pos = ['PosA', 'PosN']\n",
    "    if df['Condition1'] in pos:\n",
    "        return 1\n",
    "    if df['Condition2'] in pos:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Creating near Positive Feature column\n",
    "housing['NearPos'] = housing.apply(near_pos, axis = 1)\n",
    "print(housing.shape)\n",
    "# Creating function to see if Condition1 or Condition2 shows house is adjacent to arterial road\n",
    "def near_art(df):\n",
    "    art = ['Artery']\n",
    "    if df['Condition1'] in art:\n",
    "        return 1\n",
    "    if df['Condition2'] in art:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Creating adjacent to arterial road column\n",
    "housing['Artery'] = housing.apply(near_art, axis = 1)\n",
    "print(housing.shape)\n",
    "# Function to converting ordinal KitchenQual to number\n",
    "def qual_to_num_kit(df):\n",
    "    if df['KitchenQual'] == 'Po':\n",
    "        return 1\n",
    "    if df['KitchenQual'] == 'Fa':\n",
    "        return 2\n",
    "    if df['KitchenQual'] == 'TA':\n",
    "        return 3\n",
    "    if df['KitchenQual'] == 'Gd':\n",
    "        return 4\n",
    "    if df['KitchenQual'] == 'Ex':\n",
    "        return 5\n",
    "\n",
    "# Replacing Kitchen Qual string values with numerical\n",
    "housing['KitchenQual'] = housing.apply(qual_to_num_kit, axis = 1)\n",
    "print(housing.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# separate dummy df\n",
    "dum_bldgtype = pd.get_dummies(housing.BldgType, prefix='BldgType')\n",
    "dum_bldgtype.drop('BldgType_'+str(housing['BldgType'].mode()[0]), axis=1, inplace=True)\n",
    "housing = pd.concat([housing, dum_bldgtype], axis=1)\n",
    "print(housing.shape)\n",
    "\n",
    "# House Style\n",
    "\n",
    "housing['HouseStyle'].replace('2.5Fin', '2Story', inplace=True)\n",
    "housing['HouseStyle'].replace('2.5Unf', '1Story', inplace=True)\n",
    "housing['HouseStyle'].replace('1.5Unf', '1.5Fin', inplace=True)\n",
    "\n",
    "dum_housestyle = pd.get_dummies(housing.HouseStyle, prefix='HouseStyle')\n",
    "dum_housestyle.drop('HouseStyle_'+str(housing['HouseStyle'].mode()[0]), axis=1, inplace=True)\n",
    "# concatenating dum_housestyle with train\n",
    "housing = pd.concat([housing, dum_housestyle], axis=1)\n",
    "print(housing.shape)\n",
    "\n",
    "housing.shape\n",
    "\n",
    "housing.columns\n",
    "\n",
    "coldrop = ['MSSubClass']\n",
    "housing = housing.drop(coldrop, axis = 1)\n",
    "\n",
    "housing.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(housing.SalePrice, bins = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(housing.SalePrice, hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"SalePrice\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the log function to make the data normal\n",
    "plt.hist(np.log(housing.SalePrice+1), bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.log(housing.SalePrice+1), hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(housing[\"SalePrice\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the different types of foundations\n",
    "#print(housing.Foundation.value_counts())\n",
    "#sns.countplot(housing.Foundation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to converting ordinal KitchenQual to number\n",
    "def qual_to_num_kit(df):\n",
    "    if df['KitchenQual'] == 'Po':\n",
    "        return 1\n",
    "    if df['KitchenQual'] == 'Fa':\n",
    "        return 2\n",
    "    if df['KitchenQual'] == 'TA':\n",
    "        return 3\n",
    "    if df['KitchenQual'] == 'Gd':\n",
    "        return 4\n",
    "    if df['KitchenQual'] == 'Ex':\n",
    "        return 5\n",
    "# Replacing Kitchen Qual string values with numerical\n",
    "housing['KitchenQual'] = housing.apply(qual_to_num_kit, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keep the numerical data to the left and categorical data to the right.\n",
    "# Visualise the proportion of each categorical labels\n",
    "categorical_data=[]\n",
    "housing_new =pd.DataFrame()\n",
    "#print(len(housing_new))\n",
    "housing_new[\"SalePrice\"]=housing[\"SalePrice\"]\n",
    "for ele in housing.columns:\n",
    "    if np.dtype(housing[ele])== \"int64\" or np.dtype(housing[ele])==\"float64\":\n",
    "        housing_new[ele] = housing[ele]\n",
    "        print(len(housing_new))\n",
    "    else:\n",
    "        categorical_data.append(ele)\n",
    "        sns.countplot(housing[ele])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in categorical_data:\n",
    "    print(name, ': number of values', len(housing[name].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in categorical_data:\n",
    "    housing_new[ele] = housing[ele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the dummies of each categorical Data.\n",
    "for ele in categorical_data:\n",
    "    # Converting type of columns to category\n",
    "    housing_new=pd.get_dummies(housing_new, prefix=\"{}_\".format(ele), \n",
    "                            columns=[ele], \n",
    "                            drop_first=True)\n",
    "    \n",
    "\n",
    "housing_new#=housing_new.drop([\"repeat\",\"repeat_x\", \"repeat_y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting and Evaluating Multiple Linear Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_m= np.array(housing_new.iloc[:1500,1:])\n",
    "y_m = np.array(housing_new.iloc[:1500, 0])\n",
    "x_t= np.array(housing_new.iloc[1500:,1:])\n",
    "y_t = np.array(housing_new.iloc[1500:, 0])\n",
    "ols.fit(x_m, y_m)\n",
    "print(\"beta_1, beta_2: \" + str(np.round(ols.coef_, 3)))\n",
    "print(\"beta_0: \" + str(np.round(ols.intercept_, 3)))\n",
    "print(\"RSS: %.2f\" % np.sum((ols.predict(x_m) - y_m) ** 2))\n",
    "print(\"R^2: %.5f\" % ols.score(x_t, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression \n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_new.iloc[:,1:], housing_new.iloc[:,0], test_size=0.5, random_state=0)\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "print(\"R^2 for train set: %f\" %ols.score(X_train, y_train))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print(\"R^2 for test  set: %f\" %ols.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression \n",
    "lst =[]\n",
    "test_r =[]\n",
    "train_r=[]\n",
    "\n",
    "for ele in housing_new.iloc[:,1:].columns:\n",
    "    ols = LinearRegression()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(housing_new[[ele]], housing_new.iloc[:,0], test_size=0.5, random_state=0)\n",
    "    lst.append(ele)\n",
    "    ols.fit(X_train, y_train)\n",
    "    #print('-'*50)\n",
    "    #print(ele.upper())\n",
    "    train_r.append(ols.score(X_train, y_train))\n",
    "\n",
    "   \n",
    "\n",
    "    test_r.append(ols.score(X_test, y_test))\n",
    "    #print('-'*50)\n",
    "feature_importance =pd.DataFrame( {\"element\":lst, \"train_r\":train_r,\"test_r\":test_r}).sort_values(by=\"train_r\")[::-1]\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the contribution of each feature by importance\n",
    "housingimp=pd.DataFrame()\n",
    "housingimp[\"SalePrice\"]=housing.SalePrice\n",
    "housingimp[list(feature_importance.element)]=housing_new[list(feature_importance.element)]\n",
    "    \n",
    "lst =[]\n",
    "test_r =[]\n",
    "train_r=[]\n",
    "\n",
    "for ele in range(2,len(housingimp.columns)):\n",
    "    ols = LinearRegression()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(housingimp.iloc[:,1:ele], housing_new.iloc[:,0], test_size=0.5, random_state=0)\n",
    "    lst.append(housingimp.iloc[:,1:ele].columns)\n",
    "    ols.fit(X_train, y_train)\n",
    "    #print('-'*50)\n",
    "    \n",
    "    train_r.append(ols.score(X_train, y_train))\n",
    "\n",
    "    test_r.append(ols.score(X_test, y_test))\n",
    "    #print('-'*50)\n",
    "nfeature_importance =pd.DataFrame( {\"element\":lst, \"train_r\":train_r,\"test_r\":test_r}).sort_values(by=\"train_r\")\n",
    "nfeature_importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(nfeature_importance[\"train_r\"], bins=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoxCox Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the distributions are not normal and would affect the performance of our model. \n",
    "We do a box cox tranformation to  make it normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the boxcox transform of numerical dtypes\n",
    "lst = list(housing_new.columns)[1:38]\n",
    "for ele in lst:\n",
    "    print(ele)\n",
    "    fitted_data, fitted_lambda = stats.boxcox(housing_new[ele]+1)\n",
    "    housing_new[\"Log_{}\".format(ele)]=fitted_data\n",
    "    print(fitted_lambda)\n",
    "housing_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_new.iloc[:,38:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression \n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_new.iloc[:,1:300], housing_new.iloc[:,0], test_size=0.3, random_state=0)\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "print(\"R^2 for train set: %f\" %ols.score(X_train, y_train))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print(\"R^2 for test  set: %f\" %ols.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data using the standard scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "scaled = pd.DataFrame(scaler.fit_transform(housing_new.iloc[:,1:]))\n",
    "scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled, housing_new.iloc[:,0], test_size=0.3, random_state=0)\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "print(\"R^2 for train set: %f\" %ols.score(X_train, y_train))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print(\"R^2 for test  set: %f\" %ols.score(X_test, y_test))\n",
    "# This model does not persorm well on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N =len(housingimp.columns)\n",
    "breaks = range(1,N, 2) # Deternine the number of columns to run PCA with.\n",
    "pca=PCA(n_components=2)\n",
    "exp_ratio = []\n",
    "# We will not use the longitude, latitude and price in the PCA to make sure that our result is blind to locations and price.\n",
    "data =housing_new[[\"SalePrice\"]] # We assume that longitude, latiude are independent predictors.\n",
    "pca.set_params(n_components=2)\n",
    "\n",
    "for i in range(len(breaks)-1):\n",
    "    principal_components_ = pca.fit_transform(housing_new.iloc[:,breaks[i]:breaks[i+1]])\n",
    "    \n",
    "\n",
    "    data1 =housing_new[[\"SalePrice\"]]\n",
    "    # Visualize data across the linear components\n",
    "     # Create a new dataframe for the PCA values\n",
    " \n",
    "    total_var = sum(pca.explained_variance_ratio_)*100\n",
    "    data1[\"PCA_{}\".format(i)] =   list(principal_components_[:,0]) # Add the first pricipal component to the data1\n",
    "    \n",
    "    exp_ratio.append(pca.explained_variance_ratio_[0]) \n",
    "    \n",
    "    data[\"PCA_\"+\"{}\".format(i)] = list(principal_components_[:,0])\n",
    "    if pca.explained_variance_ratio_[1]>0.25:\n",
    "        data[\"PCA_2\"+\"{}\".format(i)] = list(principal_components_[:,1])\n",
    "        exp_ratio.append(pca.explained_variance_ratio_[1])\n",
    "    \n",
    "    #fig = plt.figure(figsize=(15, 10))\n",
    "    #fig = px.scatter_3d(\n",
    "        #np.array(data1), x=0, y=1, z=2, color=gadf3['price'],\n",
    "        #title=f'Total Explained Variance: {total_var:.2f}%,  PCA_1:  {100*pca.explained_variance_ratio_[0]:.2f}%, PCA_2: {100*pca.explained_variance_ratio_[1]:.2f}% ',\n",
    "        #labels={\"Longitude\", \"Latitude\", \"PCA_\"}\n",
    "\n",
    "    #)\n",
    "    #fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,1:], housing_new.iloc[:,0], test_size=0.3, random_state=0)\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "print(\"R^2 for train set: %f\" %ols.score(X_train, y_train))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print(\"R^2 for test  set: %f\" %ols.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do **multiple linear regression** with a new data set.\n",
    "- Report the coefficient of determination from the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression \n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_new.iloc[:,1:300], housing_new.iloc[:,0], test_size=0.3, random_state=0)\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "print(\"R^2 for train set: %f\" %ols.score(X_train, y_train))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print(\"R^2 for test  set: %f\" %ols.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the second DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging original DF with additional dataset\n",
    "geodata = housing_new.merge(realEstate, left_on = 'PID', right_on = 'MapRefNo')\n",
    "geodata = geodata[['PID','Prop_Addr','MA_Zip1']]\n",
    "print(geodata.shape)\n",
    "geodata.head(10)\n",
    "\n",
    "hs_na = geodata.isna().sum()\n",
    "hs_na.plot.bar()\n",
    "hs_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing additional libraries for further geographical analysis\n",
    "import geopy\n",
    "from geopy import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# initializing locator and geocode\n",
    "breaks = range(0,2600, 100) \n",
    "locator = Nominatim(user_agent=\"myGeocoder\")\n",
    "geocode = RateLimiter(locator.geocode, min_delay_seconds=1.3)\n",
    "\n",
    "# Creating full address column (merging local address with 'Ames, IA')\n",
    "geodata.head()\n",
    "geodata['Full_Adr'] = geodata['Prop_Addr'] + ', Ames, IA'\n",
    "geodata.head()\n",
    "\n",
    "# Creating subsets of larger df to make geopy work more traceable\n",
    "\n",
    "# Creating subsets of larger df to make geopy work more traceable\n",
    "df1 = geodata.iloc[:100, :]\n",
    "df2 = geodata.iloc[100:200, :]\n",
    "df3 = geodata.iloc[200:300, :]\n",
    "df4 = geodata.iloc[300:400, :]\n",
    "df5 = geodata.iloc[400:500, :]\n",
    "df6 = geodata.iloc[500:600, :]\n",
    "df7 = geodata.iloc[600:700, :]\n",
    "df8 = geodata.iloc[700:800, :]\n",
    "df9 = geodata.iloc[800:900, :]\n",
    "df10 = geodata.iloc[900:1000, :]\n",
    "df11 = geodata.iloc[1000:1100, :]\n",
    "df12 = geodata.iloc[1100:1200, :]\n",
    "df13 = geodata.iloc[1200:1300, :]\n",
    "df14 = geodata.iloc[1300:1400, :]\n",
    "df15 = geodata.iloc[1400:1500, :]\n",
    "df16 = geodata.iloc[1500:1600, :]\n",
    "df17 = geodata.iloc[1600:1700, :]\n",
    "df18 = geodata.iloc[1700:1800, :]\n",
    "df19 = geodata.iloc[1800:1900, :]\n",
    "df20 = geodata.iloc[1900:2000, :]\n",
    "df21 = geodata.iloc[2000:2100, :]\n",
    "df22 = geodata.iloc[2100:2200, :]\n",
    "df23 = geodata.iloc[2200:2300, :]\n",
    "df24 = geodata.iloc[2300:2400, :]\n",
    "df25 = geodata.iloc[2400:2500, :]\n",
    "df26 = geodata.iloc[2500:, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Applying geocode lat/long function to subset DFs made in above code\n",
    "df1['gcode'] = df1['Full_Adr'].apply(geocode)\n",
    "print(\"df1 done\")\n",
    "df2['gcode'] = df2['Full_Adr'].apply(geocode)\n",
    "print(\"df2 done\")\n",
    "df3['gcode'] = df3['Full_Adr'].apply(geocode)\n",
    "print(\"df3 done\")\n",
    "df4['gcode'] = df4['Full_Adr'].apply(geocode)\n",
    "print(\"df4 done\")\n",
    "df5['gcode'] = df5['Full_Adr'].apply(geocode)\n",
    "print(\"df5 done\")\n",
    "df6['gcode'] = df6['Full_Adr'].apply(geocode)\n",
    "print(\"df6 done\")\n",
    "df7['gcode'] = df7['Full_Adr'].apply(geocode)\n",
    "print(\"df7 done\")\n",
    "df8['gcode'] = df8['Full_Adr'].apply(geocode)\n",
    "print(\"df8 done\")\n",
    "df9['gcode'] = df9['Full_Adr'].apply(geocode)\n",
    "print(\"df9 done\")\n",
    "df10['gcode'] = df10['Full_Adr'].apply(geocode)\n",
    "print(\"df10 done\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df11['gcode'] = df11['Full_Adr'].apply(geocode)\n",
    "print(\"df11 done\")\n",
    "df12['gcode'] = df12['Full_Adr'].apply(geocode)\n",
    "print(\"df12 done\")\n",
    "df13['gcode'] = df13['Full_Adr'].apply(geocode)\n",
    "print(\"df13 done\")\n",
    "df14['gcode'] = df14['Full_Adr'].apply(geocode)\n",
    "print(\"df14 done\")\n",
    "df15['gcode'] = df15['Full_Adr'].apply(geocode)\n",
    "print(\"df15 done\")\n",
    "df16['gcode'] = df16['Full_Adr'].apply(geocode)\n",
    "print(\"df16 done\")\n",
    "df17['gcode'] = df17['Full_Adr'].apply(geocode)\n",
    "print(\"df17 done\")\n",
    "df18['gcode'] = df18['Full_Adr'].apply(geocode)\n",
    "print(\"df18 done\")\n",
    "df19['gcode'] = df19['Full_Adr'].apply(geocode)\n",
    "print(\"df19 done\")\n",
    "df20['gcode'] = df20['Full_Adr'].apply(geocode)\n",
    "print(\"df20 done\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df21['gcode'] = df21['Full_Adr'].apply(geocode)\n",
    "print(\"df21 done\")\n",
    "df22['gcode'] = df22['Full_Adr'].apply(geocode)\n",
    "print(\"df22 done\")\n",
    "df23['gcode'] = df23['Full_Adr'].apply(geocode)\n",
    "print(\"df23 done\")\n",
    "df24['gcode'] = df24['Full_Adr'].apply(geocode)\n",
    "print(\"df24 done\")\n",
    "df25['gcode'] = df25['Full_Adr'].apply(geocode)\n",
    "print(\"df25 done\")\n",
    "df26['gcode'] = df26['Full_Adr'].apply(geocode)\n",
    "print(\"df26 done\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Breaking out lat/long into separate columns\n",
    "df1['lat'] = df1['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df1['long'] = df1['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df2['lat'] = df2['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df2['long'] = df2['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df3['lat'] = df3['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df3['long'] = df3['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df4['lat'] = df4['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df4['long'] = df4['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df5['lat'] = df5['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df5['long'] = df5['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df6['lat'] = df6['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df6['long'] = df6['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df7['lat'] = df7['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df7['long'] = df7['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df8['lat'] = df8['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df8['long'] = df8['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df9['lat'] = df9['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df9['long'] = df9['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df10['lat'] = df10['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df10['long'] = df10['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df11['lat'] = df11['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df11['long'] = df11['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df12['lat'] = df12['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df12['long'] = df12['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df13['lat'] = df13['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df13['long'] = df13['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df14['lat'] = df14['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df14['long'] = df14['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df15['lat'] = df15['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df15['long'] = df15['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df16['lat'] = df16['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df16['long'] = df16['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df17['lat'] = df17['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df17['long'] = df17['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df18['lat'] = df18['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df18['long'] = df18['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df19['lat'] = df19['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df19['long'] = df19['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df20['lat'] = df20['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df20['long'] = df20['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "\n",
    "df21['lat'] = df21['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df21['long'] = df21['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df22['lat'] = df22['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df22['long'] = df22['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df23['lat'] = df23['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df23['long'] = df23['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df24['lat'] = df24['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df24['long'] = df24['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df25['lat'] = df25['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df25['long'] = df25['gcode'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "df26['lat'] = df26['gcode'].apply(lambda loc: loc.latitude if loc else None)\n",
    "df26['long'] = df26['gcode'].apply(lambda loc: loc.longitude if loc else None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Combining subsetted DFs back into one\n",
    "df = df1.append(df2.append(df3.append(df4.append(df5.append(df6.append(df7.append(df8.append(df9.append(df10.append(df11.append(df12.append(df13.append(df14.append(df15.append(df16)))))))))))))))\n",
    "df.shape\n",
    "\n",
    "df = df.append(df17.append(df18.append(df19.append(df20.append(df21.append(df22.append(df23.append(df24.append(df25.append(df26))))))))))\n",
    "df.shape\n",
    "geodata = df\n",
    "geodata.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# function to get distance between coordinates of address and Iowa State University\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "def get_dist(lat, long):\n",
    "    R = 6373.0\n",
    "    lat1 = radians(42.0267)\n",
    "    long1 = radians(-93.6465)\n",
    "    dlon = radians(long) - long1\n",
    "    dlat = radians(lat) - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(radians(lat)) * cos(lat1) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Applying distance to ISU fct to all rows\n",
    "geodata['dist'] = geodata.apply(lambda x: get_dist(x['lat'], x['long']), axis=1)\n",
    "geodata.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Merging geodata\n",
    "housing_new = housing_new.merge(geodata,left_on = 'PID', right_on = 'PID')\n",
    "# Dropping duplicate PIDs\n",
    "housing_new = housing_new.drop_duplicates(subset = 'PID')\n",
    "housing_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census =pd.read_csv(\"Cenus_data.csv\")\n",
    "geocode = pd.read_csv(\"GeocodeResults2.csv\")\n",
    "geocode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
